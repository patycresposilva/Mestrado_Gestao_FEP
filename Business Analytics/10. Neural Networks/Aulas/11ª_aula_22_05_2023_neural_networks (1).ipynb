{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural networks - Bank marketing campaign"
      ],
      "metadata": {
        "id": "up8xetFoVb5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHARACTERISTICS OF THE MODEL:**\n",
        "\n",
        "* the model behind chat gpt\n",
        "\n",
        "* they work  by trying to initiating the human brain\n",
        "\n",
        "* each circle is a neuron\n",
        "\n",
        "* in this example, we have a model with two predictors (X1, X2) and one outcome variable (Y)\n",
        "\n",
        "* if it is a binary `classification` problem, the values of X1, X2 flow through the network and is output as probability of being of a class 1.\n",
        "\n",
        "* if it is a `regression` problem, Y is a prediction.\n",
        "\n",
        "* we have 3 types of layer:\n",
        " \n",
        "    1. **input layer**: it is when the predictors enter the network;\n",
        "\n",
        "    2. **hidden layer**: transform (mathematically) the values of the predictors; we can have several hidden layers and each one of them can have as many neurons as we want (we can choose the number of hidden layers and also the number of neurons in each layer) => they will be a hyperparameter\n",
        "\n",
        "    3. **output layer**: we get the predictors\n",
        "  \n",
        "NOTE: the higer the number of hidden layers and the number of neurons in each layer, the more complex model the NN can learn.\n",
        "\n",
        "Examples of neutral networks:\n",
        "teta (Θ) - bias\n",
        "w - weights\n",
        "\n",
        "**WEIGHTS AND BIAS:**\n",
        "\n",
        "* each wwight is associated with an arrow;\n",
        "\n",
        "* wi,j is the weight associated to the arrow connecting neuron i and neuron j\n",
        "\n",
        "\n",
        "**WHAT IS THE INPUT AT NEURON 3?**\n",
        "\n",
        "(ver tablet)\n",
        "\n",
        "Inside a neuron in the hidden layers, its input undergoes a transformation\n",
        "\n",
        " - this transformation is done by an **`activation function`**\n",
        " - there are many activation functions:\n",
        "      1. **logistic function** - tends to work fine with binary classification problems (between 0 and 1)\n",
        "      2. **tahn function** -  (between -1 and 1)\n",
        "      3. **ReLU function** - \n",
        "\n",
        "In order to have the output of a certain neuron we need to apply the activation function to the formulas we have written (tablet).\n",
        "\n",
        "INPUT:\n",
        "\n",
        "w1,3X1 + w2,3X2 + Θ3\n",
        "\n",
        "OUTPUT: (`f` means the activation function)\n",
        "\n",
        "f(w1,3X1 + w2,3X2 + Θ3)\n",
        "\n",
        "If the `activation function` is the `logistic function`, so the outcome will be [(1)/(1+e^(w1,3X1 + w2,3X2 + Θ3))] (see tablet)\n",
        "\n",
        "This model can run very complex problems...\n",
        "\n",
        "\n",
        "**NETWORK LEARNING:**\n",
        "\n",
        "- the learning of the network consists in finding the optimal values for the weights and bias;\n",
        "\n",
        "- what is the output of neuron 3 when considering the logistic activation function?\n",
        "\n",
        "- it stats by going random the weights and biases.\n",
        "\n",
        "- to see how good are the weights and biases, we have to get and idea of how wrong are the predictors (loss function)\n",
        "\n",
        "**LOSS FUNCTION:**\n",
        "\n",
        "We define a loss function for that:\n",
        "\n",
        "  * for regression, the usual loss function is MSE (Mean squared error)\n",
        "  * for classification, the usual loss function is `cross-entropy`:\n",
        "\n",
        "      - Y actual is 0 or 1 (the label of the class)\n",
        "\n",
        "      - Y predicted is the probability of being 1.\n",
        "\n",
        "\n",
        "**EXAMPLE OF LOSS FUNCTION:** (see tablet)\n",
        "\n",
        "L(w) = w2\n",
        "\n",
        "-> there are many optimization algoritms used in NN but all of them are base on the `gradient descent algorithm`.\n",
        "\n",
        "FORMULA of `gradient descent algorithm`: see tablet\n",
        "\n",
        "calculate w0, w1, w2, w3 - we are getting closer to zero...\n",
        "\n",
        "w0 = 1\n",
        "\n",
        "w1 = 0.8\n",
        "\n",
        "w2 = 0.64\n",
        "\n",
        "w3 = 0.512\n",
        "\n",
        "and so on...\n",
        "\n",
        "WHAT IS THE OPTIMAL/DESIRED VALUE FOR THE `LEARNING RATE`?\n"
      ],
      "metadata": {
        "id": "CoutmtntmPhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 1\n",
        "eta = 0.1\n",
        "\n",
        "n_iter = 100\n",
        "\n",
        "for i in range(n_iter):\n",
        "  w = w - eta*2*w\n",
        "\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RviXyfxnydG4",
        "outputId": "ad37152c-c4a7-4f3e-d347-237be6d3eed2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0370359763344877e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see this number is very close to zero - convergence!\n",
        "\n",
        "The higher the learning rate the faster the convergence, it will go much faster! If we use a learning rate to high we may lose the convergence! Higher learning rate accelerate the convergence and we can lose the convergence.\n",
        "\n",
        "- with too high learning rates, we may loose convergence!!! The usual is 0.1, but since it is a hyperparameter we can search for the best value!\n"
      ],
      "metadata": {
        "id": "KdNsgWRhy3H1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2332895c-2818-494c-ab64-70b10c5a0f3d",
        "id": "VdCLRBSkztyV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy= 0.9160242792109257\n",
            "[[ 1879  1821]\n",
            " [  946 28304]]\n",
            "Precision= 0.6651327433628319\n",
            "Recall= 0.5078378378378379\n",
            "Accuracy= 0.9129643117261471\n",
            "[[ 468  472]\n",
            " [ 245 7053]]\n",
            "Precision= 0.6563814866760168\n",
            "Recall= 0.4978723404255319\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "df = pd.read_csv('/content/bank_mark_campaign.csv', sep=';')\n",
        "\n",
        "df = df.replace('unknown', np.nan) \n",
        "\n",
        "col_nan = df.columns[df.isna().any(axis=0)].to_list()\n",
        "col_num = df.describe().columns.to_list()\n",
        "df.columns.difference(col_nan + col_num)\n",
        "col_cat = df.columns.difference(col_nan + col_num + ['y']).to_list()\n",
        "\n",
        "na_treat = Pipeline([\n",
        "    ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "    ('oneh', OneHotEncoder(drop='first'))])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('na_tr', na_treat, col_nan),\n",
        "    ('cat_tr', OneHotEncoder(drop='first'), col_cat),\n",
        "    ('scale_tr', StandardScaler(), col_num)], \n",
        "    remainder='passthrough')\n",
        "\n",
        "# hyper = {\n",
        "#     'ccp_alpha': [0.001, 0.01, 0.1, 0.2, 0.5]\n",
        "# }\n",
        "\n",
        "# pipe = Pipeline([\n",
        "#     ('pre', preprocessor),\n",
        "#     ('grid', GridSearchCV(DecisionTreeClassifier(class_weight=\"balanced\"), hyper, cv=5, scoring='roc_auc'))])\n",
        "\n",
        "pipe = Pipeline([\n",
        "     ('pre', preprocessor),\n",
        "     ('nn', MLPClassifier(hidden_layer_sizes=(5,3)))])\n",
        "\n",
        "#using two hidden layers with 5 and 3 neurons, respectively. \n",
        "\n",
        "X = df.drop('y', axis=1)\n",
        "y = df['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# train\n",
        "y_pred = pipe.predict(X_train)\n",
        "\n",
        "acur = accuracy_score(y_train, y_pred)\n",
        "print(f'Accuracy= {acur}')\n",
        "cm = confusion_matrix(y_train, y_pred, labels=['yes', 'no'])\n",
        "print(cm)\n",
        "precision = precision_score(y_train, y_pred, pos_label='yes')\n",
        "print(f'Precision= {precision}')\n",
        "recall = recall_score(y_train, y_pred, pos_label='yes')\n",
        "print(f'Recall= {recall}')\n",
        "\n",
        "# test\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "acur = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy= {acur}')\n",
        "cm = confusion_matrix(y_test, y_pred, labels=['yes', 'no'])\n",
        "print(cm)\n",
        "precision = precision_score(y_test, y_pred, pos_label='yes')\n",
        "print(f'Precision= {precision}')\n",
        "recall = recall_score(y_test, y_pred, pos_label='yes')\n",
        "print(f'Recall= {recall}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the classes are too imbalanced (the yes one is very under-represented), we need to compensate for that.\n",
        "\n",
        "Also, they advise the neural networks tend to overfit and we need to use a sort of `lasso` regularization to avoid overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "i3BdaYMm1t8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if there is something that we can do to prevent overfitting!"
      ],
      "metadata": {
        "id": "A9giuCr75zIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usual the `L2 regularization` -> 0.0001."
      ],
      "metadata": {
        "id": "4XGZhWI-54Y_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac30226-5035-4150-e9bd-4595a3d7cfe8",
        "id": "CXBY5je35s4k"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy= 0.9157511380880121\n",
            "[[ 1729  1971]\n",
            " [  805 28445]]\n",
            "Precision= 0.6823204419889503\n",
            "Recall= 0.4672972972972973\n",
            "Accuracy= 0.907623209516873\n",
            "[[ 400  540]\n",
            " [ 221 7077]]\n",
            "Precision= 0.644122383252818\n",
            "Recall= 0.425531914893617\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "df = pd.read_csv('/content/bank_mark_campaign.csv', sep=';')\n",
        "\n",
        "df = df.replace('unknown', np.nan) \n",
        "\n",
        "col_nan = df.columns[df.isna().any(axis=0)].to_list()\n",
        "col_num = df.describe().columns.to_list()\n",
        "df.columns.difference(col_nan + col_num)\n",
        "col_cat = df.columns.difference(col_nan + col_num + ['y']).to_list()\n",
        "\n",
        "na_treat = Pipeline([\n",
        "    ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "    ('oneh', OneHotEncoder(drop='first'))])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('na_tr', na_treat, col_nan),\n",
        "    ('cat_tr', OneHotEncoder(drop='first'), col_cat),\n",
        "    ('scale_tr', StandardScaler(), col_num)], \n",
        "    remainder='passthrough')\n",
        "\n",
        "hyper = {\n",
        "     'alpha': [0.001, 0.01, 0.2]\n",
        "}\n",
        "\n",
        "# pipe = Pipeline([\n",
        "#     ('pre', preprocessor),\n",
        "#     ('grid', GridSearchCV(DecisionTreeClassifier(class_weight=\"balanced\"), hyper, cv=5, scoring='roc_auc'))])\n",
        "\n",
        "#pipe = Pipeline([\n",
        "#     ('pre', preprocessor),\n",
        "#     ('nn', MLPClassifier(hidden_layer_sizes=(5,3)))])\n",
        "\n",
        "#using two hidden layers with 5 and 3 neurons, respectively. \n",
        "\n",
        "pipe = Pipeline([\n",
        "     ('pre', preprocessor),\n",
        "     ('grid', GridSearchCV(MLPClassifier(hidden_layer_sizes=(5,3)), hyper, cv=5, scoring='roc_auc'))])\n",
        "\n",
        "X = df.drop('y', axis=1)\n",
        "y = df['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# train\n",
        "y_pred = pipe.predict(X_train)\n",
        "\n",
        "acur = accuracy_score(y_train, y_pred)\n",
        "print(f'Accuracy= {acur}')\n",
        "cm = confusion_matrix(y_train, y_pred, labels=['yes', 'no'])\n",
        "print(cm)\n",
        "precision = precision_score(y_train, y_pred, pos_label='yes')\n",
        "print(f'Precision= {precision}')\n",
        "recall = recall_score(y_train, y_pred, pos_label='yes')\n",
        "print(f'Recall= {recall}')\n",
        "\n",
        "# test\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "acur = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy= {acur}')\n",
        "cm = confusion_matrix(y_test, y_pred, labels=['yes', 'no'])\n",
        "print(cm)\n",
        "precision = precision_score(y_test, y_pred, pos_label='yes')\n",
        "print(f'Precision= {precision}')\n",
        "recall = recall_score(y_test, y_pred, pos_label='yes')\n",
        "print(f'Recall= {recall}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks in sklearn does not have class_weights parameter.\n",
        "\n",
        "So we need to use `random oversampling`.\n",
        "\n",
        "80% \"no\" -> 1000 samples\n",
        "\n",
        "20% \"yes\" -> oversampling (randomly) to get 1000 samples as well."
      ],
      "metadata": {
        "id": "jEua45dC7CAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0bf033e-28c0-4639-815c-41218958868f",
        "id": "Ku6J360g8Sq4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy= 0.8463732928679818\n",
            "[[ 3515   185]\n",
            " [ 4877 24373]]\n",
            "Precision= 0.4188512869399428\n",
            "Recall= 0.95\n",
            "Accuracy= 0.838553046856033\n",
            "[[ 874   66]\n",
            " [1264 6034]]\n",
            "Precision= 0.4087932647333957\n",
            "Recall= 0.9297872340425531\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "df = pd.read_csv('/content/bank_mark_campaign.csv', sep=';')\n",
        "\n",
        "df = df.replace('unknown', np.nan) \n",
        "\n",
        "col_nan = df.columns[df.isna().any(axis=0)].to_list()\n",
        "col_num = df.describe().columns.to_list()\n",
        "df.columns.difference(col_nan + col_num)\n",
        "col_cat = df.columns.difference(col_nan + col_num + ['y']).to_list()\n",
        "\n",
        "na_treat = Pipeline([\n",
        "    ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "    ('oneh', OneHotEncoder(drop='first'))])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('na_tr', na_treat, col_nan),\n",
        "    ('cat_tr', OneHotEncoder(drop='first'), col_cat),\n",
        "    ('scale_tr', StandardScaler(), col_num)], \n",
        "    remainder='passthrough')\n",
        "\n",
        "hyper = {\n",
        "     'alpha': [0.001, 0.01, 0.2]\n",
        "}\n",
        "\n",
        "pipe = Pipeline([\n",
        "    # ('pre', preprocessor),\n",
        "    ('grid', GridSearchCV(MLPClassifier(hidden_layer_sizes=(5,3)), hyper, cv=5, scoring='roc_auc'))])\n",
        "\n",
        "X = df.drop('y', axis=1)\n",
        "y = df['y']\n",
        "\n",
        "#CHange here also - Important- We don't need to use the preprocessor in the final pipeline\n",
        "X= preprocessor.fit_transform(X) # this transforms the columns outside the pipeline\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "\n",
        "\n",
        "ros= RandomOverSampler(random_state=45) #this creates the sampler\n",
        "\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train,y_train) #this does the oversampling on the training set\n",
        "\n",
        "\n",
        "pipe.fit(X_train_res, y_train_res)  #this applies the model to the resampled training set\n",
        "\n",
        "\n",
        "# train\n",
        "y_pred = pipe.predict(X_train)\n",
        "\n",
        "acur = accuracy_score(y_train, y_pred)\n",
        "print(f'Accuracy= {acur}')\n",
        "cm = confusion_matrix(y_train, y_pred, labels=['yes', 'no'])\n",
        "print(cm)\n",
        "precision = precision_score(y_train, y_pred, pos_label='yes')\n",
        "print(f'Precision= {precision}')\n",
        "recall = recall_score(y_train, y_pred, pos_label='yes')\n",
        "print(f'Recall= {recall}')\n",
        "\n",
        "# test\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "acur = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy= {acur}')\n",
        "cm = confusion_matrix(y_test, y_pred, labels=['yes', 'no'])\n",
        "print(cm)\n",
        "precision = precision_score(y_test, y_pred, pos_label='yes')\n",
        "print(f'Precision= {precision}')\n",
        "recall = recall_score(y_test, y_pred, pos_label='yes')\n",
        "print(f'Recall= {recall}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision measure of predictive performance:**\n",
        "\n",
        "precision = (852)/(1106+852) => rate of true \"yes\" from the cases predicted as \"yes\""
      ],
      "metadata": {
        "id": "h238PmhBAK0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural networks - Data car radios"
      ],
      "metadata": {
        "id": "5-UwK5T2VTId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import datetime\n",
        "\n",
        "df = pd.read_excel('/content/data_carradios.xlsx')\n",
        "\n",
        "def get_ages(col):\n",
        "  result = (datetime.datetime.now()-col).astype('<m8[Y]')\n",
        "  result = pd.DataFrame(result)\n",
        "  return result\n",
        "\n",
        "ager = Pipeline([\n",
        "    ('ages', FunctionTransformer(get_ages, feature_names_out='one-to-one')),\n",
        "    ('scale', StandardScaler())\n",
        "])\n",
        "\n",
        "def get_weekdays(col):\n",
        "  result = col.iloc[:,0].dt.weekday\n",
        "  result = pd.DataFrame(result)\n",
        "  return result\n",
        "\n",
        "weeker = Pipeline([\n",
        "    ('weekd', FunctionTransformer(get_weekdays, feature_names_out='one-to-one')),\n",
        "    ('oneh', OneHotEncoder(drop='first'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('ages_tr', ager, ['bdate']),\n",
        "    ('weekd_tr', weeker, ['datep']),\n",
        "    ('team_tr', OneHotEncoder(drop='first'), ['team']),\n",
        "    ('scaler', StandardScaler(), ['prized', 'prizeq'])],\n",
        "    remainder='passthrough')\n",
        "\n",
        "X = df.drop('perc_defec', axis=1)\n",
        "y = df['perc_defec']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    #('lm', LinearRegression())\n",
        "    ('nn', MLPRegressor(max_iter=10000))]) #se fizessemos sem max_iter=10000 ia dar uma nota qualquer sobre a convergência - ver!!\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# train\n",
        "y_pred = pipe.predict(X_train)\n",
        "\n",
        "mae = mean_absolute_error(y_train, y_pred)\n",
        "rsme = mean_squared_error(y_train, y_pred, squared=False)\n",
        "r2 = r2_score(y_train, y_pred)\n",
        "\n",
        "print(f'MAE= {mae}')\n",
        "print(f'RSME= {rsme}')\n",
        "print(f'R2= {r2}')\n",
        "\n",
        "# test\n",
        "y_pred = pipe.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rsme = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'MAE= {mae}')\n",
        "print(f'RSME= {rsme}')\n",
        "print(f'R2= {r2}')"
      ],
      "metadata": {
        "id": "OdmJO58ofCeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c39b1f-cdb2-4b45-89e4-1ee127db3b02"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE= 2.9266816849059865\n",
            "RSME= 3.8893617156162876\n",
            "R2= 0.9334782758299508\n",
            "MAE= 3.818436934336518\n",
            "RSME= 5.123978439401416\n",
            "R2= 0.8884905512941925\n"
          ]
        }
      ]
    }
  ]
}